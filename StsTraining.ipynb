{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cher-liang/Gravitas-NLP\n",
    "%cd Gravitas-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o dataset.zip \"https://drive.google.com/uc?id=12LAWEMQpGCxkFQbZFRN6_v8imQkg40rp\"\n",
    "!unzip dataset.zip\n",
    "!rm dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\Gravitas-NLP\\CustomCrossEncoder.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# from torch.optim import Optimizer\n",
    "\n",
    "# from tqdm.autonotebook import tqdm, trange\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "# import transformers\n",
    "\n",
    "import os\n",
    "import logging \n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Type\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from CustomCrossEncoder import CrossEncoder\n",
    "from CustomEvaluator import CECustomEvaluator\n",
    "# from CustomSTSBert import BertLSTMClassificationLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, \n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StsData:\n",
    "    def __init__(self, row) -> None:\n",
    "        self.sentence_pair = (row[\"reference_answer\"],row[\"answer\"])\n",
    "        self.score = row[\"normalized_score\"]\n",
    "        self.dataset = row[\"source\"]\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        string = \"Sentence Pair: {}\\n\".format(self.sentence_pair)\n",
    "        string += \"Score: {}\\t Dataset: {}\\n\".format(self.score,self.dataset)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataFrameWrapper:\n",
    "    dataset_source: str\n",
    "    df: pd.DataFrame\n",
    "\n",
    "\n",
    "dataset_names = [\"asap_sas\", \"cunlp\", \"misc\", \"sag\", \"semeval\", \"stita\"]\n",
    "dataframes: List[DataFrameWrapper] = []\n",
    "for dataset_name in dataset_names:\n",
    "    dataframes.append(\n",
    "        DataFrameWrapper(\n",
    "            dataset_source=dataset_name,\n",
    "            df=pd.read_excel(f\"datasets/{dataset_name}.xlsx\"),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets: Dict[str,List[StsData]]={}\n",
    "for dataframe in dataframes:\n",
    "    datasets[dataframe.dataset_source]=dataframe.df.apply(StsData, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset for Semeval\n",
    "test_semeval_df = pd.read_excel(\"datasets/test/semeval_unseen_domains.xlsx\")\n",
    "dev_semeval_df1 = pd.read_excel(\"datasets/develop/semeval_unseen_answers.xlsx\")\n",
    "dev_semeval_df2 = pd.read_excel(\"datasets/develop/semeval_unseen_questions.xlsx\")\n",
    "\n",
    "test_semeval_dataset = test_semeval_df.apply(StsData, axis=1).tolist()\n",
    "dev_semeval_dataset = (\n",
    "    dev_semeval_df1.apply(StsData, axis=1).tolist()\n",
    "    + dev_semeval_df2.apply(StsData, axis=1).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names.remove(\"semeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts_model_name = \"cross-encoder/stsb-distilroberta-base\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(sts_model_name)\n",
    "# config = AutoConfig.from_pretrained(sts_model_name)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(sts_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -9999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 22:22:24 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "# print(model.model.base_model)\n",
    "\n",
    "# print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=16\n",
    "num_epochs = 20\n",
    "model_save_path = 'output/training_sts_continue_training-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c9465dd6194a16a6254f2ad6ecb4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b5f62658045998987dbdf22e9ae25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m CECustomEvaluator\u001b[38;5;241m.\u001b[39mfrom_input_examples(dev_dataset, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msts-dev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m num_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m#10% of train data for warm-up\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m test_evaluator \u001b[38;5;241m=\u001b[39m  CECustomEvaluator\u001b[38;5;241m.\u001b[39mfrom_input_examples(test_dataset, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msts-test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m test_evaluator(model, output_path\u001b[38;5;241m=\u001b[39mmodel_save_path)\n",
      "File \u001b[1;32md:\\Dev\\Gravitas-NLP\\CustomCrossEncoder.py:289\u001b[0m, in \u001b[0;36mCrossEncoder.fit\u001b[1;34m(self, train_dataloader, evaluator, freeze, epochs, criterion, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar)\u001b[0m\n\u001b[0;32m    287\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    288\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m criterion(logits, scores)\n\u001b[1;32m--> 289\u001b[0m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    293\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Dev\\Gravitas-NLP\\.venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\Gravitas-NLP\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5 Fold Cross Validation with (60-20-20, Train-Develop-Test split)\n",
    "kfSplits = [\n",
    "    kf.split(dataset) for (key, dataset) in datasets.items() if key != \"semeval\"\n",
    "]\n",
    "for fold, indexes in enumerate(zip(*kfSplits)):\n",
    "    train_datasets_list, test_datasets_list, dev_datasets_list = (\n",
    "        [np.array(datasets[\"semeval\"])],\n",
    "        [np.array(test_semeval_dataset)],\n",
    "        [np.array(dev_semeval_dataset)],\n",
    "    )\n",
    "\n",
    "    # Split each datasets into train, develop, test except \"semeval\" dataset\n",
    "    for (train_test_index, dev_index), dataset_name in zip(indexes, dataset_names):\n",
    "        train_index, test_index = train_test_split(train_test_index, test_size=0.25)\n",
    "        train_datasets_list.append(np.take(datasets[dataset_name], train_index))\n",
    "        dev_datasets_list.append(np.take(datasets[dataset_name], dev_index))\n",
    "        test_datasets_list.append(np.take(datasets[dataset_name], test_index))\n",
    "\n",
    "    train_dataset = torch.utils.data.ConcatDataset(train_datasets_list)\n",
    "    dev_dataset = torch.utils.data.ConcatDataset(dev_datasets_list)\n",
    "    test_dataset = torch.utils.data.ConcatDataset(test_datasets_list)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    evaluator = CECustomEvaluator.from_input_examples(dev_dataset, name=\"sts-dev\")\n",
    "\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "    model.fit(train_dataloader=train_dataloader,\n",
    "            evaluator=evaluator,\n",
    "            epochs=num_epochs,\n",
    "            evaluation_steps=1000,\n",
    "            warmup_steps=warmup_steps,\n",
    "            output_path=model_save_path)\n",
    "    \n",
    "    test_evaluator =  CECustomEvaluator.from_input_examples(test_dataset, name='sts-test')\n",
    "    test_evaluator(model, output_path=model_save_path)\n",
    "\n",
    "\n",
    "    # dev_dataloader = DataLoader(\n",
    "    #     dataset=dev_dataset,\n",
    "    #     batch_size=batch_size,\n",
    "    #     # collate_fn=smart_batching_collate,\n",
    "    #     shuffle=True,\n",
    "    # )\n",
    "\n",
    "    # test_dataloader = DataLoader(\n",
    "    #     dataset=test_dataset,\n",
    "    #     batch_size=batch_size,\n",
    "    #     # collate_fn=smart_batching_collate,\n",
    "    #     shuffle=True,\n",
    "    # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
