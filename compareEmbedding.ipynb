{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cher-liang/Gravitas-NLP\n",
    "%cd Gravitas-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p datasets/semeval2013-Task7-5way/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Gravitas-NLP/datasets/semeval2013-Task7-5way/processed\n",
    "!curl -L -o dataset.zip \"https://drive.google.com/uc?id=12LAWEMQpGCxkFQbZFRN6_v8imQkg40rp\"\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd /content/Gravitas-NLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data import torch_dataset\n",
    "from basis.enums import TrainingType\n",
    "from basis import config\n",
    "from pytorch_model import QuestionAnswerPairSimilarityModel\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from typing import Dict, Type, Callable, List\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_device = torch.device(device)\n",
    "\n",
    "max_length: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_model_name = \"cross-encoder/qnli-distilroberta-base\"\n",
    "sts_model_name = \"cross-encoder/stsb-distilroberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(qnli_model_name)\n",
    "model_config = AutoConfig.from_pretrained(qnli_model_name)\n",
    "\n",
    "# qnli_bert_model = AutoModelForSequenceClassification.from_pretrained(qnli_model_name)\n",
    "# sts_bert_model = AutoModelForSequenceClassification.from_pretrained(sts_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smart_batching_collate_text_only(batch):\n",
    "#     texts = [[] for _ in range(len(batch[0]))]\n",
    "\n",
    "#     for example in batch:\n",
    "#         for idx, text in enumerate(example):\n",
    "#             texts[idx].append(text.strip())\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         *texts, padding=True, truncation=\"longest_first\", return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     for name in tokenized:\n",
    "#         tokenized[name] = tokenized[name].to(target_device)\n",
    "\n",
    "#     return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smart_batching_collate(batch):\n",
    "#     texts = [[] for _ in range(len(batch[0].texts))]\n",
    "#     labels = []\n",
    "\n",
    "#     for example in batch:\n",
    "#         for idx, text in enumerate(example.texts):\n",
    "#             texts[idx].append(text.strip())\n",
    "\n",
    "#         labels.append(example.label)\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         *texts,\n",
    "#         padding=True,\n",
    "#         truncation=\"longest_first\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=max_length\n",
    "#     )\n",
    "\n",
    "#     labels = torch.tensor(labels, dtype=torch.float).to(target_device)\n",
    "\n",
    "#     for name in tokenized:\n",
    "#         tokenized[name] = tokenized[name].to(target_device)\n",
    "\n",
    "#     return tokenized, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate_new(batch):\n",
    "    ques_ref_ans_texts = [[] for _ in range(len(batch[0].ques_ref_ans_texts))]\n",
    "    ques_stud_ans_texts = [[] for _ in range(len(batch[0].ques_stud_ans_texts))]\n",
    "    ref_ans_stud_ans_texts = [[] for _ in range(len(batch[0].ref_ans_stud_ans_texts))]\n",
    "    scores = []\n",
    "\n",
    "    for example in batch:\n",
    "        for idx, (\n",
    "            ques_ref_ans_text,\n",
    "            ques_stud_ans_text,\n",
    "            ref_ans_stud_ans_text,\n",
    "        ) in enumerate(\n",
    "            zip(\n",
    "                example.ques_ref_ans_texts,\n",
    "                example.ques_stud_ans_texts,\n",
    "                example.ref_ans_stud_ans_texts,\n",
    "            )\n",
    "        ):\n",
    "            ques_ref_ans_texts[idx].append(ques_ref_ans_text.strip())\n",
    "            ques_stud_ans_texts[idx].append(ques_stud_ans_text.strip())\n",
    "            ref_ans_stud_ans_texts[idx].append(ref_ans_stud_ans_text.strip())\n",
    "\n",
    "        scores.append(example.scores)\n",
    "\n",
    "    ques_ref_tokenized = tokenizer(\n",
    "        *ques_ref_ans_texts,\n",
    "        padding=True,\n",
    "        truncation=\"longest_first\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "    ques_stud_tokenized = tokenizer(\n",
    "        *ques_stud_ans_texts,\n",
    "        padding=True,\n",
    "        truncation=\"longest_first\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "    ref_stud_tokenized = tokenizer(\n",
    "        *ref_ans_stud_ans_texts,\n",
    "        padding=True,\n",
    "        truncation=\"longest_first\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    scores = torch.tensor(scores, dtype=torch.float).to(target_device)\n",
    "\n",
    "    for ques_ref_name, ques_stud_name, ref_stud_name in zip(\n",
    "        ques_ref_tokenized, ques_stud_tokenized, ref_stud_tokenized\n",
    "    ):\n",
    "        ques_ref_tokenized[ques_ref_name] = ques_ref_tokenized[ques_ref_name].to(target_device)\n",
    "        ques_stud_tokenized[ques_stud_name] = ques_stud_tokenized[ques_stud_name].to(target_device)\n",
    "        ref_stud_tokenized[ref_stud_name] = ref_stud_tokenized[ref_stud_name].to(target_device)\n",
    "    \n",
    "    return ques_ref_tokenized, ques_stud_tokenized, ref_stud_tokenized, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GravitasData:\n",
    "    def __init__(self, row) -> None:\n",
    "        self.ques_ref_ans_texts=[row[\"question_text\"],row[\"best_match_reference_answer\"]]\n",
    "        self.ques_stud_ans_texts=[row[\"question_text\"],row[\"answer_text\"]]\n",
    "        self.ref_ans_stud_ans_texts=[row[\"best_match_reference_answer\"],row[\"answer_text\"]]\n",
    "        self.scores=row[\"normalized_scores\"]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        string = \"Question & Ref Ans: {}\\n\".format(self.ques_ref_ans_texts)\n",
    "        string += \"Question & Student Ans: {}\\n\".format(self.ques_stud_ans_texts)\n",
    "        string += \"Ref Ans & Student Ans: {}\\n\".format(self.ref_ans_stud_ans_texts)\n",
    "        string += \"Scores: {}\\n\".format(self.scores)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\Gravitas-NLP\\basis\\..\\datasets\\semeval2013-Task7-5way\\processed\n"
     ]
    }
   ],
   "source": [
    "root_path = Path(config.ROOT_PATH) / \"datasets\" / \"semeval2013-Task7-5way\" / \"processed\"\n",
    "\n",
    "training_dataset = torch_dataset.GravitasDataset(root_path)\n",
    "eval_dataset = torch_dataset.GravitasDataset(root_path,train=TrainingType.TESTING_UNSEEN_ANSWERS)\n",
    "test_dataset = torch_dataset.GravitasDataset(root_path,train=TrainingType.TESTING_UNSEEN_QUESTIONS)\n",
    "\n",
    "training_data = training_dataset.data.apply(GravitasData,axis=1).tolist()\n",
    "eval_data = eval_dataset.data.apply(GravitasData,axis=1).tolist()\n",
    "test_data = test_dataset.data.apply(GravitasData,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=QuestionAnswerPairSimilarityModel(256,qnli_model_name=qnli_model_name,sts_model_name=sts_model_name)\n",
    "model = QuestionAnswerPairSimilarityModel(768,256,qnli_model_name,sts_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=32, collate_fn=smart_batching_collate_new, shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_data, batch_size=32, collate_fn=smart_batching_collate_new, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=32, collate_fn=smart_batching_collate_new, shuffle=True\n",
    ")\n",
    "\n",
    "# iterator = tqdm(train_dataloader, desc=\"Batches\")\n",
    "# for ques_ref_features, ques_stud_features, ref_stud_features, scores in iterator:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the loss function\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Choose the optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Number of epochs\n",
    "# epochs = 100\n",
    "\n",
    "# # Initialize variables for early stopping\n",
    "# patience = 10\n",
    "# best_val_loss = None\n",
    "# epochs_no_improve = 0\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(epochs):\n",
    "#     # Training phase\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_data, 0):\n",
    "#         inputs1, inputs2, labels = data\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs1, inputs2)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # Validation phase\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_data:\n",
    "#             inputs1, inputs2, labels = data\n",
    "#             outputs = model(inputs1, inputs2)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if best_val_loss is None or val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         epochs_no_improve = 0\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "#         if epochs_no_improve == patience:\n",
    "#             print('Early stopping!')\n",
    "#             break\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, scheduler: str, warmup_steps: int, t_total: int):\n",
    "    \"\"\"\n",
    "    Returns the correct learning rate scheduler. Available scheduler: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
    "    \"\"\"\n",
    "    scheduler = scheduler.lower()\n",
    "    if scheduler == 'constantlr':\n",
    "        return transformers.get_constant_schedule(optimizer)\n",
    "    elif scheduler == 'warmupconstant':\n",
    "        return transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "    elif scheduler == 'warmuplinear':\n",
    "        return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "    elif scheduler == 'warmupcosine':\n",
    "        return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "    elif scheduler == 'warmupcosinewithhardrestarts':\n",
    "        return transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scheduler {}\".format(scheduler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(path, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves all model and tokenizer to path\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        return\n",
    "\n",
    "    logger.info(\"Save model to {}\".format(path))\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_during_training(evaluator, output_path, save_best_model, epoch, steps, callback, best_score):\n",
    "    \"\"\"Runs evaluation during the training\"\"\"\n",
    "    score = evaluator(output_path=output_path, epoch=epoch, steps=steps)\n",
    "    if callback is not None:\n",
    "        callback(score, epoch, steps)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        if save_best_model:\n",
    "            save(output_path)\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_dataloader: DataLoader,\n",
    "    eval_dataloader: DataLoader,\n",
    "    # evaluator: SentenceEvaluator = None,\n",
    "    epochs: int = 1,\n",
    "    criterion=None,\n",
    "    activation_fct=nn.Identity(),\n",
    "    scheduler: str = \"WarmupLinear\",\n",
    "    warmup_steps: int = 10000,\n",
    "    optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n",
    "    optimizer_params: Dict[str, object] = {\"lr\": 2e-5},\n",
    "    weight_decay: float = 0.01,\n",
    "    # evaluation_steps: int = 0,\n",
    "    output_path: str = None,\n",
    "    # save_best_model: bool = True,\n",
    "    max_grad_norm: float = 1,\n",
    "    # callback: Callable[[float, int, int], None] = None,\n",
    "    show_progress_bar: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with the given training objective\n",
    "    Each training objective is sampled in turn for one batch.\n",
    "    We sample only as many batches from each objective as there are in the smallest one\n",
    "    to make sure of equal training with each dataset.\n",
    "\n",
    "    :param train_dataloader: DataLoader with training InputExamples\n",
    "    :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n",
    "    :param epochs: Number of epochs for training\n",
    "    :param criterion: Which loss function to use for training. If None, will use nn.BCEWithLogitsLoss() if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n",
    "    :param activation_fct: Activation function applied on top of logits output of model.\n",
    "    :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
    "    :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n",
    "    :param optimizer_class: Optimizer\n",
    "    :param optimizer_params: Optimizer parameters\n",
    "    :param weight_decay: Weight decay for model parameters\n",
    "    :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n",
    "    :param output_path: Storage path for the model and evaluation files\n",
    "    :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n",
    "    :param max_grad_norm: Used for gradient normalization.\n",
    "    :param callback: Callback function that is invoked after each evaluation.\n",
    "            It must accept the following three parameters in this order:\n",
    "            `score`, `epoch`, `steps`\n",
    "    :param show_progress_bar: If True, output a tqdm progress bar\n",
    "    \"\"\"\n",
    "    # train_dataloader.collate_fn = self.smart_batching_collate\n",
    "\n",
    "    # Automatic Mixed Precision\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model.to(target_device)\n",
    "\n",
    "    if output_path is not None:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # best_score = -9999999\n",
    "    num_train_steps = int(len(train_dataloader) * epochs)\n",
    "\n",
    "    # Prepare optimizers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
    "\n",
    "    if isinstance(scheduler, str):\n",
    "        scheduler = get_scheduler(\n",
    "            optimizer,\n",
    "            scheduler=scheduler,\n",
    "            warmup_steps=warmup_steps,\n",
    "            t_total=num_train_steps,\n",
    "        )\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    skip_scheduler = False\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    patience = 10\n",
    "    best_val_loss = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
    "        # training_steps = 0\n",
    "        model.zero_grad()\n",
    "        model.train()\n",
    "\n",
    "        for ques_ref_features, ques_stud_features, ref_stud_features, scores in tqdm(\n",
    "            train_dataloader,\n",
    "            desc=\"Iteration\",\n",
    "            smoothing=0.05,\n",
    "            disable=not show_progress_bar,\n",
    "        ):\n",
    "            # if use_amp:\n",
    "            with autocast():\n",
    "                model_predictions = model(\n",
    "                    ques_ref_features,\n",
    "                    ques_stud_features,\n",
    "                    ref_stud_features,\n",
    "                    # return_dict=True\n",
    "                )\n",
    "                logits = activation_fct(model_predictions)\n",
    "                logits = logits.view(-1)\n",
    "                loss_value = criterion(logits, scores)\n",
    "\n",
    "            scale_before_step = scaler.get_scale()\n",
    "            scaler.scale(loss_value).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            skip_scheduler = scaler.get_scale() != scale_before_step\n",
    "            # else:\n",
    "            #     model_predictions = self.model(**features, return_dict=True)\n",
    "            #     logits = activation_fct(model_predictions.logits)\n",
    "            #     if self.config.num_labels == 1:\n",
    "            #         logits = logits.view(-1)\n",
    "            #     loss_value = criterion(logits, labels)\n",
    "            #     loss_value.backward()\n",
    "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "            #     optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if not skip_scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for ques_ref_features, ques_stud_features, ref_stud_features, scores in eval_dataloader: \n",
    "                model_predictions = model(\n",
    "                    ques_ref_features,\n",
    "                    ques_stud_features,\n",
    "                    ref_stud_features,\n",
    "                    # return_dict=True\n",
    "                )\n",
    "                logits = activation_fct(model_predictions)\n",
    "                logits = logits.view(-1)\n",
    "                loss = criterion(logits, scores)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if best_val_loss is None or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "        #     training_steps += 1\n",
    "\n",
    "        #     if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n",
    "        #         best_score = eval_during_training(\n",
    "        #             evaluator,\n",
    "        #             output_path,\n",
    "        #             save_best_model,\n",
    "        #             epoch,\n",
    "        #             training_steps,\n",
    "        #             callback,\n",
    "        #             best_score,\n",
    "        #         )\n",
    "\n",
    "        #         model.zero_grad()\n",
    "        #         model.train()\n",
    "\n",
    "        # if evaluator is not None:\n",
    "        #     best_score=eval_during_training(\n",
    "        #         evaluator, output_path, save_best_model, epoch, -1, callback, best_score\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339658409c644c948a12c74b6d9cad4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fe14689c4b42adabc4c69246bd9da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4202, 0.4617, 0.5190, 0.4209, 0.4414, 0.4758, 0.5024, 0.4053, 0.4639,\n",
      "        0.5190, 0.5278, 0.5137, 0.3728, 0.4514, 0.4763, 0.3313, 0.4583, 0.3938,\n",
      "        0.4287, 0.3879, 0.4553, 0.4243, 0.4700, 0.4478, 0.4326, 0.4534, 0.3804,\n",
      "        0.3914, 0.4053, 0.4302, 0.4021, 0.3660], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ViewBackward0>) tensor([1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "        0.5000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 1.0000, 0.5000, 0.5000,\n",
      "        1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.0000, 1.0000,\n",
      "        0.5000, 1.0000, 1.0000, 0.5000, 0.5000], device='cuda:0')\n",
      "tensor([0.4133, 0.4072, 0.4587, 0.5288, 0.4478, 0.5259, 0.4128, 0.4739, 0.3879,\n",
      "        0.4751, 0.4043, 0.4058, 0.3923, 0.4106, 0.3813, 0.4995, 0.4622, 0.3613,\n",
      "        0.4172, 0.4753, 0.4783, 0.4973, 0.3235, 0.3777, 0.3892, 0.4241, 0.5195,\n",
      "        0.4541, 0.4260, 0.4807, 0.4412, 0.3621], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ViewBackward0>) tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 1.0000, 0.5000, 0.5000,\n",
      "        0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000,\n",
      "        0.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 0.0000,\n",
      "        1.0000, 0.0000, 0.5000, 0.5000, 1.0000], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m num_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      4\u001b[0m )  \u001b[38;5;66;03m# 10% of train data for warm-up\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 127\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_dataloader, eval_dataloader, epochs, criterion, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, output_path, max_grad_norm, show_progress_bar)\u001b[0m\n\u001b[0;32m    124\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m criterion(logits, scores)\n\u001b[0;32m    126\u001b[0m scale_before_step \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[1;32m--> 127\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    129\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[1;32md:\\Dev\\Gravitas-NLP\\.venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\Gravitas-NLP\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "warmup_steps = math.ceil(\n",
    "    len(train_dataloader) * num_epochs * 0.1\n",
    ")  # 10% of train data for warm-up\n",
    "\n",
    "fit(\n",
    "    model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    epochs=100,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_dataloader = DataLoader(\n",
    "#     question_answer,\n",
    "#     batch_size=32,\n",
    "#     collate_fn=smart_batching_collate_text_only,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "# iterator = tqdm(inp_dataloader, desc=\"Batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 56, 768])\n",
      "torch.Size([32, 47, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 47, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 39, 768])\n",
      "torch.Size([32, 40, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 38, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 64, 768])\n",
      "torch.Size([32, 59, 768])\n",
      "torch.Size([32, 73, 768])\n",
      "torch.Size([32, 72, 768])\n",
      "torch.Size([32, 96, 768])\n",
      "torch.Size([32, 67, 768])\n",
      "torch.Size([32, 42, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 39, 768])\n",
      "torch.Size([32, 72, 768])\n",
      "torch.Size([32, 87, 768])\n",
      "torch.Size([32, 57, 768])\n",
      "torch.Size([32, 71, 768])\n",
      "torch.Size([32, 25, 768])\n",
      "torch.Size([32, 22, 768])\n",
      "torch.Size([32, 39, 768])\n",
      "torch.Size([32, 40, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 39, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 54, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 46, 768])\n",
      "torch.Size([32, 45, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 44, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 38, 768])\n",
      "torch.Size([32, 71, 768])\n",
      "torch.Size([32, 33, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 30, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 28, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 44, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 36, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 31, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 25, 768])\n",
      "torch.Size([32, 36, 768])\n",
      "torch.Size([32, 28, 768])\n",
      "torch.Size([32, 36, 768])\n",
      "torch.Size([32, 35, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 42, 768])\n",
      "torch.Size([32, 42, 768])\n",
      "torch.Size([32, 47, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 30, 768])\n",
      "torch.Size([32, 33, 768])\n",
      "torch.Size([32, 76, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 105, 768])\n",
      "torch.Size([32, 87, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 52, 768])\n",
      "torch.Size([32, 56, 768])\n",
      "torch.Size([32, 46, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 66, 768])\n",
      "torch.Size([32, 47, 768])\n",
      "torch.Size([32, 33, 768])\n",
      "torch.Size([32, 46, 768])\n",
      "torch.Size([32, 39, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 43, 768])\n",
      "torch.Size([32, 36, 768])\n",
      "torch.Size([32, 33, 768])\n",
      "torch.Size([32, 40, 768])\n",
      "torch.Size([32, 35, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 61, 768])\n",
      "torch.Size([32, 26, 768])\n",
      "torch.Size([32, 47, 768])\n",
      "torch.Size([32, 29, 768])\n",
      "torch.Size([32, 38, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 24, 768])\n",
      "torch.Size([32, 32, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 27, 768])\n",
      "torch.Size([32, 36, 768])\n",
      "torch.Size([32, 37, 768])\n",
      "torch.Size([32, 31, 768])\n",
      "torch.Size([32, 34, 768])\n",
      "torch.Size([32, 29, 768])\n",
      "torch.Size([32, 38, 768])\n",
      "torch.Size([32, 29, 768])\n",
      "torch.Size([32, 25, 768])\n",
      "torch.Size([32, 33, 768])\n",
      "torch.Size([32, 30, 768])\n",
      "torch.Size([32, 31, 768])\n",
      "torch.Size([32, 55, 768])\n",
      "torch.Size([32, 54, 768])\n",
      "torch.Size([32, 145, 768])\n",
      "torch.Size([32, 135, 768])\n",
      "torch.Size([32, 65, 768])\n",
      "torch.Size([32, 129, 768])\n",
      "torch.Size([32, 91, 768])\n",
      "torch.Size([32, 70, 768])\n",
      "torch.Size([32, 89, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 67, 768])\n",
      "torch.Size([32, 85, 768])\n",
      "torch.Size([32, 69, 768])\n",
      "torch.Size([32, 126, 768])\n",
      "torch.Size([32, 170, 768])\n",
      "torch.Size([32, 119, 768])\n",
      "torch.Size([32, 108, 768])\n",
      "torch.Size([32, 132, 768])\n",
      "torch.Size([32, 118, 768])\n",
      "torch.Size([32, 106, 768])\n",
      "torch.Size([32, 103, 768])\n",
      "torch.Size([32, 138, 768])\n",
      "torch.Size([32, 144, 768])\n",
      "torch.Size([32, 115, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 63, 768])\n",
      "torch.Size([32, 49, 768])\n",
      "torch.Size([32, 65, 768])\n",
      "torch.Size([32, 105, 768])\n",
      "torch.Size([32, 111, 768])\n",
      "torch.Size([32, 127, 768])\n",
      "torch.Size([32, 127, 768])\n",
      "torch.Size([32, 85, 768])\n",
      "torch.Size([32, 65, 768])\n",
      "torch.Size([32, 68, 768])\n",
      "torch.Size([32, 65, 768])\n",
      "torch.Size([32, 87, 768])\n",
      "torch.Size([32, 94, 768])\n",
      "torch.Size([32, 96, 768])\n",
      "torch.Size([32, 100, 768])\n",
      "torch.Size([32, 91, 768])\n",
      "torch.Size([32, 109, 768])\n",
      "torch.Size([32, 104, 768])\n",
      "torch.Size([32, 65, 768])\n",
      "torch.Size([32, 86, 768])\n",
      "torch.Size([32, 72, 768])\n",
      "torch.Size([32, 98, 768])\n",
      "torch.Size([32, 86, 768])\n",
      "torch.Size([32, 108, 768])\n",
      "torch.Size([32, 92, 768])\n",
      "torch.Size([32, 66, 768])\n",
      "torch.Size([32, 63, 768])\n",
      "torch.Size([32, 54, 768])\n",
      "torch.Size([32, 89, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 67, 768])\n",
      "torch.Size([32, 72, 768])\n",
      "torch.Size([32, 88, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 91, 768])\n",
      "torch.Size([32, 97, 768])\n",
      "torch.Size([32, 61, 768])\n",
      "torch.Size([32, 57, 768])\n",
      "torch.Size([32, 125, 768])\n",
      "torch.Size([32, 142, 768])\n",
      "torch.Size([32, 116, 768])\n",
      "torch.Size([32, 60, 768])\n",
      "torch.Size([32, 112, 768])\n",
      "torch.Size([32, 107, 768])\n",
      "torch.Size([32, 94, 768])\n",
      "torch.Size([32, 79, 768])\n",
      "torch.Size([32, 87, 768])\n",
      "torch.Size([32, 73, 768])\n",
      "torch.Size([32, 75, 768])\n",
      "torch.Size([32, 59, 768])\n",
      "torch.Size([32, 106, 768])\n",
      "torch.Size([32, 172, 768])\n",
      "torch.Size([32, 172, 768])\n",
      "torch.Size([32, 82, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 122, 768])\n",
      "torch.Size([32, 127, 768])\n",
      "torch.Size([32, 98, 768])\n",
      "torch.Size([32, 161, 768])\n",
      "torch.Size([32, 187, 768])\n",
      "torch.Size([32, 159, 768])\n",
      "torch.Size([32, 147, 768])\n",
      "torch.Size([32, 83, 768])\n",
      "torch.Size([32, 84, 768])\n",
      "torch.Size([32, 123, 768])\n",
      "torch.Size([32, 149, 768])\n",
      "torch.Size([32, 124, 768])\n",
      "torch.Size([32, 154, 768])\n",
      "torch.Size([32, 158, 768])\n",
      "torch.Size([32, 180, 768])\n",
      "torch.Size([32, 175, 768])\n",
      "torch.Size([32, 187, 768])\n",
      "torch.Size([32, 184, 768])\n",
      "torch.Size([32, 109, 768])\n",
      "torch.Size([32, 110, 768])\n",
      "torch.Size([32, 41, 768])\n",
      "torch.Size([32, 40, 768])\n",
      "torch.Size([32, 129, 768])\n",
      "torch.Size([32, 125, 768])\n",
      "torch.Size([32, 127, 768])\n",
      "torch.Size([32, 145, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 107, 768])\n",
      "torch.Size([32, 95, 768])\n",
      "torch.Size([32, 86, 768])\n",
      "torch.Size([32, 81, 768])\n",
      "torch.Size([32, 55, 768])\n",
      "torch.Size([32, 61, 768])\n",
      "torch.Size([32, 44, 768])\n",
      "torch.Size([32, 73, 768])\n",
      "torch.Size([32, 73, 768])\n",
      "torch.Size([32, 98, 768])\n",
      "torch.Size([32, 110, 768])\n",
      "torch.Size([32, 96, 768])\n",
      "torch.Size([32, 52, 768])\n",
      "torch.Size([32, 278, 768])\n",
      "torch.Size([32, 268, 768])\n",
      "torch.Size([32, 96, 768])\n",
      "torch.Size([32, 105, 768])\n",
      "torch.Size([32, 116, 768])\n",
      "torch.Size([32, 163, 768])\n",
      "torch.Size([32, 163, 768])\n",
      "torch.Size([32, 116, 768])\n",
      "torch.Size([32, 136, 768])\n",
      "torch.Size([32, 136, 768])\n",
      "torch.Size([32, 109, 768])\n",
      "torch.Size([32, 153, 768])\n",
      "torch.Size([32, 143, 768])\n",
      "torch.Size([32, 121, 768])\n",
      "torch.Size([32, 45, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 54, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 53, 768])\n",
      "torch.Size([32, 46, 768])\n",
      "torch.Size([32, 60, 768])\n",
      "torch.Size([32, 80, 768])\n",
      "torch.Size([32, 68, 768])\n",
      "torch.Size([32, 59, 768])\n",
      "torch.Size([32, 69, 768])\n",
      "torch.Size([32, 70, 768])\n",
      "torch.Size([32, 45, 768])\n",
      "torch.Size([32, 75, 768])\n",
      "torch.Size([32, 106, 768])\n",
      "torch.Size([32, 132, 768])\n",
      "torch.Size([32, 151, 768])\n",
      "torch.Size([32, 158, 768])\n",
      "torch.Size([32, 169, 768])\n",
      "torch.Size([32, 151, 768])\n",
      "torch.Size([14, 42, 768])\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# model.to(torch.device(device))\n",
    "# with torch.no_grad():\n",
    "#     for features in iterator:\n",
    "#         qnli_predictions1 = model.base_model(**features, return_dict=True)\n",
    "#         embeddings = qnli_predictions1[0]\n",
    "\n",
    "#         print(embeddings.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
